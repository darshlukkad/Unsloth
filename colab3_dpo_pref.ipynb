{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHWJZUvxSLTd1f4xOsvIJS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshlukkad/Unsloth/blob/main/colab3_dpo_pref.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWvjRVZAIvq1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slRNsjZIDVq1",
        "outputId": "6ab488a9-f9aa-475b-cbea-dbf7583a1c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m136.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "Python: 3.12.12\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Mon Nov 10 04:39:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0             28W /   70W |     102MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "dtype: torch.float16 | device: cuda\n"
          ]
        }
      ],
      "source": [
        "# --- Install core deps (Colab) ---\n",
        "%pip -q install -U unsloth trl transformers datasets accelerate peft bitsandbytes einops evaluate sentencepiece\n",
        "\n",
        "# --- Stability flags BEFORE importing transformers/trl/peft ---\n",
        "import os, sys, platform, torch\n",
        "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"   # avoid flaky compiled kernels on some Colab builds\n",
        "os.environ[\"UNSLOTH_STABLE_DOWNLOADS\"] = \"1\"  # quieter, more robust HF downloads\n",
        "\n",
        "# Import Unsloth FIRST so it can patch transformers properly\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported, PatchDPOTrainer\n",
        "\n",
        "# Patch TRL's DPO for PEFT/LoRA-friendly reference handling\n",
        "PatchDPOTrainer()\n",
        "\n",
        "# Now import the rest\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Basic env printouts\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "!nvidia-smi || echo \"No NVIDIA GPU detected\"\n",
        "\n",
        "# Precision & common constants\n",
        "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_SEQ_LEN = 2048\n",
        "MAX_LENGTH  = 512   # prompt/completion length used by DPOTrainer config later\n",
        "print(\"dtype:\", dtype, \"| device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy model: 4-bit base + LoRA adapters (PEFT)\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "MAX_SEQ_LEN = 2048\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "# Load base in 4-bit (QLoRA style) to keep VRAM low\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name      = model_id,\n",
        "    max_seq_length  = MAX_SEQ_LEN,\n",
        "    dtype           = dtype,\n",
        "    load_in_4bit    = True,\n",
        ")\n",
        "\n",
        "# Attach LoRA adapters as our trainable policy head\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r                          = 16,\n",
        "    lora_alpha                 = 16,\n",
        "    lora_dropout               = 0.05,\n",
        "    target_modules             = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "                                  \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state               = 3407,\n",
        "    max_seq_length             = MAX_SEQ_LEN,\n",
        ")\n",
        "\n",
        "# Tokenizer safety defaults\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"Loaded policy base:\", model_id)\n",
        "print(\"LoRA config: r=16, alpha=16, dropout=0.05\")\n",
        "print(\"Device:\", model.device, \"| dtype:\", dtype, \"| 4-bit:\", True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "5246712e7ef94831a9f936c8c6a7981a",
            "52274a594b55451ca16f3122bb0b381c",
            "1bd55ba6db824091a59490673e7f5d05",
            "aa75adb16986423b9e8055aa4df44705",
            "a304b508c9694c11b28dd694a9d17050",
            "9245ebe21b524f4b9938ca8d4a163a7d",
            "6fc8910c3e174f84ada05acb3ae53558",
            "52968b67d722495b8b2ea121fd954296",
            "5d598c17d2064a1e8d9a6fc16976063a",
            "6a7f52a8b9c342f58a6e67b49c1cbfcd",
            "821aab0985d34f81802bc8d9885fd03b",
            "3b2042052975488b837651e7d30fa5cf",
            "e0052fba4bc94a9e985814dfd787ab8a",
            "45387009b5a64eddb3ced403a2171876",
            "37db92bc5da94ca28a2da1de708bd251",
            "66c278962a764d3da48eebbe33abb314",
            "bffb0c7c69ea4fcebd659fb296c43ede",
            "b635b62cd010439f880f6665d8c6d48b",
            "f240cd34a7cd451a90e596254ce2ddf2",
            "12ee7204f80340cd8507202bc31296d6",
            "315f7acc0d9b454eab098697169b8f12",
            "f6d510d0e7ce4052b35c40fa997bdf30",
            "fbcfe81e2f1b4842b4ab78e26f444b2b",
            "f5326423c14044ddb78d33ce650c84f0",
            "d993205bbd1d46a8bfe59e65d392d3a8",
            "3961d88ce1c64604bbec7c78a700946c",
            "0c4d6d887119485c94b358e965ec14d4",
            "ca1134166bca42d9b61dd000033ed870",
            "76fafdc292da47f8a4a5614c4b1da71d",
            "e51f44c02ab841dd98136f9472ca8816",
            "b8c2485ed0314a91b8c77ee9ace30627",
            "3aa8793579f44bdc9f1e3ed8ddfc457e",
            "375fee7524b04b138cb8c07639d53ea4",
            "b3eace25411d43d281bef35c3a496397",
            "3fa51c45a4174322b83d5e6cdc116e2d",
            "728cc07282db4d0c8168d6a37f533003",
            "99d19a6a087549e9a148bbfee50e6ecf",
            "641b9085a8b64bda8b4b43cd43c58f5d",
            "bf81d4c862f84900bf7e17692e1c8d08",
            "9eb798457ecf48eb90ed638b440180f7",
            "9cbd5bd44fee4360b696e469fb7856df",
            "884cd726c6ca4d42b2c3bdcbfab58baa",
            "7ca01b2c10f2485bbd5273615f79e571",
            "5c45fd9b136d4d3d91483e648460489d",
            "f8007dd3ad8d44369ceb11394ce8f40a",
            "10fbd53e5401468a95befb8105d98789",
            "c9701e54bb404cbd8431222f1f712673",
            "604040b5149b49fdba0084d27972588f",
            "cce4fd09b3e641ed8f413627c0e2a0db",
            "e4d66ad3069344809dee74fcba48d2a2",
            "8f7f11c1f7784a8eaaba787a53c1ddb0",
            "f689c4813eff410b9cee6d1a57359b27",
            "3902e02df666465dbfed426f922501bc",
            "327dc95b69db45b0ba9925a36b495a07",
            "697614b7de2d408cadb4cf59b237533d",
            "45b76d9d6e564686bd3af35663e63e55",
            "12d688213f9042e3a6a86552af84de45",
            "151f1c79d9a74b4280f4327089744156",
            "96b0cbd7bcec43bcb421bc7ccbf349cc",
            "57127111210347a1be64adaa6a7dd124",
            "e77f99ea65a148d0831579fbb93195a3",
            "3e4d5c92becf4e88982966337e7643e4",
            "df09c586990f4609935c7a40df3c2103",
            "aeca58d3c5404dcaacbdda5a05a774cc",
            "56afa8581d8b47fea888ee0e99ed4676",
            "a4612c2c28624278a2ee30a242b9aef0",
            "040da1573a4048d494c15f8dd6e61e74",
            "0726fb9e439543ceb2ee19bc97e7672b",
            "c32230963b584e3795a930a9b86e8fe0",
            "0cfc7d46b1414f8daeb249a9c2b5086e",
            "fce33925fde34413a1639a27290401b7",
            "7d3b2bc98b284eff83b0d9131fb707a3",
            "8a28fec46fcb4508bc32d4498d9deaf7",
            "bfbdaa7343bd4be48a52dc83f16884a5",
            "11dd77237e4b44a48ea89131bebfbe5e",
            "bedbab3e48c1488d90981769b3636b7b",
            "7b69a3bc74f74554a4778a5353fc140c"
          ]
        },
        "id": "SUsFN9x5DX5w",
        "outputId": "5c55acbb-c6e9-4138-f701-9a1d3216e323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5246712e7ef94831a9f936c8c6a7981a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b2042052975488b837651e7d30fa5cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbcfe81e2f1b4842b4ab78e26f444b2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3eace25411d43d281bef35c3a496397"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8007dd3ad8d44369ceb11394ce8f40a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45b76d9d6e564686bd3af35663e63e55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "040da1573a4048d494c15f8dd6e61e74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.11.2 patched 30 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded policy base: HuggingFaceTB/SmolLM2-135M\n",
            "LoRA config: r=16, alpha=16, dropout=0.05\n",
            "Device: cuda:0 | dtype: torch.float16 | 4-bit: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build (prompt, chosen, rejected) from question + answers list using pm_score\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "# Pull a manageable slice for the demo\n",
        "raw = load_dataset(\"HuggingFaceH4/stack-exchange-preferences\", split=\"train[:5000]\")\n",
        "\n",
        "def _clean(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = str(s).replace(\"\\r\", \" \").replace(\"\\t\", \" \").strip()\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "def to_pairs(example):\n",
        "    q = _clean(example.get(\"question\", \"\"))\n",
        "    answers = example.get(\"answers\", [])\n",
        "    # Need at least 2 answers with different pm_score\n",
        "    scored = []\n",
        "    for a in answers:\n",
        "        txt = _clean(a.get(\"text\", \"\"))\n",
        "        pm = a.get(\"pm_score\", None)\n",
        "        if txt and pm is not None:\n",
        "            scored.append((pm, txt))\n",
        "    # dedupe identical texts to reduce trivial pairs\n",
        "    seen = set()\n",
        "    uniq = []\n",
        "    for pm, txt in scored:\n",
        "        if txt not in seen:\n",
        "            seen.add(txt)\n",
        "            uniq.append((pm, txt))\n",
        "    # sample a pair with different scores\n",
        "    random.shuffle(uniq)\n",
        "    for i in range(len(uniq)):\n",
        "        for j in range(i + 1, len(uniq)):\n",
        "            pm_i, txt_i = uniq[i]\n",
        "            pm_j, txt_j = uniq[j]\n",
        "            if pm_i != pm_j:\n",
        "                # higher score = chosen\n",
        "                if pm_i > pm_j:\n",
        "                    chosen, rejected = txt_i, txt_j\n",
        "                else:\n",
        "                    chosen, rejected = txt_j, txt_i\n",
        "                prompt = f\"Question:\\n{q}\\n\\nAnswer:\"\n",
        "                return {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
        "    # no valid pair\n",
        "    return {\"prompt\": None, \"chosen\": None, \"rejected\": None}\n",
        "\n",
        "pairs = raw.map(to_pairs, remove_columns=raw.column_names)\n",
        "pairs = pairs.filter(lambda ex: ex[\"prompt\"] is not None)\n",
        "\n",
        "# Keep a small subset for quick DPO training; increase for real runs\n",
        "SUBSET = 1000\n",
        "small_ds = pairs.select(range(min(SUBSET, len(pairs))))\n",
        "\n",
        "print(\"Raw size:\", len(raw))\n",
        "print(\"Pairs built:\", len(pairs))\n",
        "print(\"Training subset size:\", len(small_ds))\n",
        "if len(small_ds) > 0:\n",
        "    demo = small_ds[0]\n",
        "    print(\"\\nSample PROMPT:\\n\", demo[\"prompt\"][:300], \"...\\n\")\n",
        "    print(\"Sample CHOSEN:\\n\", demo[\"chosen\"][:300], \"...\\n\")\n",
        "    print(\"Sample REJECTED:\\n\", demo[\"rejected\"][:300], \"...\\n\")\n",
        "else:\n",
        "    print(\"No valid pairs found in this slice; try increasing the slice (e.g., train[:20000]).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376,
          "referenced_widgets": [
            "29db721286b046ea8347bee9dec5c6b2",
            "a0f4625ed15d4656847069dac454b3e7",
            "29041f6456d14f5794840f77c81a82f8",
            "d2d7c3eb9c264beea7791802c887729f",
            "cafe90c1751041ef829befd816c99e6c",
            "6329b8c76fb34cf88ff2feb5c33e1978",
            "55f98f31d01041368d7d387297fd0aa3",
            "5a97daeb036e49fa9b7db61b44964a7c",
            "08b4fe0e77f443e7be7834a47fd3d1eb",
            "2609a9469b524743b4e2f55f2c600242",
            "e5057fed85e142a5a1db2406222dfabe",
            "21513f31a0364e6f91fa58cadb80dfc2",
            "96d742779fad49dba016ef431c80a203",
            "22764bd29970451a9ed9275c8dba67ff",
            "9d9d8e104a8c49e8b4a3c4d64b3e4840",
            "19c341d474e247a08c4f2e2eee57ed56",
            "0b50bdcd171d4838a3885d14c6c64375",
            "ba88d3c423984d85a28bbbe659c597ff",
            "744c639a6a304fc9aee7e51484318855",
            "fbe3d42e5cba4570b3c9878dbefa1469",
            "1e12773a50424d13b1b1f14cac1e26e8",
            "e0e255844d034115aea627630299a7e6",
            "d6d6fe15f77a452cab8a75313386d162",
            "dd36a15eb7a64490a69ca45079b1eb3e",
            "e24d117109df495eac73310edd23f3d2",
            "25205c8ad8604afcbdd064a80b662f7d",
            "b51aecdb83714a16949f2e8d9eb8b774",
            "554d5f95f252404faf614af4779ebf85",
            "e279bc595ea343618dfae66db187449c",
            "9c623b0df01f43f684364598b783d669",
            "efb632e238c2488481cfef4e35f2baf2",
            "3e4c4cfe4c68408cbd2f23e460450ef8",
            "8ad9ce9527c7473683236d982d91ec43"
          ]
        },
        "id": "_s-KFoMjEI2g",
        "outputId": "98214f56-e406-47d4-b1a9-b8718a87d56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/758 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29db721286b046ea8347bee9dec5c6b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21513f31a0364e6f91fa58cadb80dfc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6d6fe15f77a452cab8a75313386d162"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw size: 5000\n",
            "Pairs built: 4808\n",
            "Training subset size: 1000\n",
            "\n",
            "Sample PROMPT:\n",
            " Question:\n",
            "<p>I have been wanting to learn about 3D printing a long time so I really want this site to succeed but I have no previous experience with the subject. </p> <p>I was wondering how can I help the site at this early stage. I thought about asking about how to get started with 3D printing but  ...\n",
            "\n",
            "Sample CHOSEN:\n",
            " <h1>Vote!</h1> <p>Private Betas love, love, <em>love</em> votes. Without votes, it's difficult to attain privileges, get rewards, and help push us out to public beta.</p> <h1>Ask Questions!</h1> <p>I know you said this:</p> <blockquote> <p>I thought about asking about how to get started with 3D prin ...\n",
            "\n",
            "Sample REJECTED:\n",
            " <p>That's the goal of the site, learn, research and ask.</p> <p>While you learn, you can always perform other tasks such as:</p> <ul> <li>improve quality posts by proposing edits,</li> <li>be active in meta (propose new ideas or write your opinion which are always welcomed),</li> <li>review <a href= ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DPO training: use DPOConfig (not TrainingArguments) so Unsloth's patch sees padding_value, etc.\n",
        "import torch\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# Turn off cache during training on some models\n",
        "if hasattr(model.config, \"use_cache\"):\n",
        "    model.config.use_cache = False\n",
        "\n",
        "pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "args = DPOConfig(\n",
        "    output_dir=\"outputs_dpo_smollm2\",\n",
        "    per_device_train_batch_size=2,     # if OOM: set to 1 and raise grad_accum\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,                # increase for real training\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    fp16=(dtype == torch.float16),\n",
        "    bf16=(dtype == torch.bfloat16),\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "\n",
        "    # --- Important for Unsloth's DPO patch / default collator ---\n",
        "    max_length=MAX_LENGTH,                         # total length (prompt+response)\n",
        "    max_prompt_length=min(256, MAX_LENGTH // 2),   # prompt cap\n",
        "    padding_value=pad_id,                          # used by DPODataCollatorWithPadding\n",
        "    label_pad_token_id=-100,                       # standard LM loss ignore index\n",
        "    truncation_mode=\"keep_end\",\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,                 # PatchDPOTrainer will handle reference efficiently\n",
        "    args=args,\n",
        "    beta=0.1,                       # typical range: 0.1‚Äì0.5\n",
        "    train_dataset=small_ds,         # built in Cell 3: (prompt, chosen, rejected)\n",
        "    tokenizer=tokenizer,\n",
        "    processing_class=tokenizer,     # make the processor explicit\n",
        ")\n",
        "\n",
        "train_out = trainer.train()\n",
        "print(\"Training complete.\")\n",
        "print(train_out)\n",
        "\n",
        "# Save ONLY the LoRA adapter (compact)\n",
        "adapter_dir = \"smollm2_dpo_adapter\"\n",
        "trainer.model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)\n",
        "print(\"Saved DPO LoRA adapter ->\", adapter_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759,
          "referenced_widgets": [
            "4e725475f7fc4d38b89c5319d1b6d976",
            "0a0861205273495c8ea0b8720fd6e3d3",
            "832d1f171c3a47d3a2443a440154cb19",
            "a2d77695a2424c60a3effc0919e424b8",
            "859d89e8d20c46209c7bc821f134344f",
            "882414659fea4969a5128679008b839c",
            "b4ff80b70c924d7d8d8c4b6a1f606e38",
            "6d5638dc24114f45984ca76a646eef0a",
            "04b15850f85f4f6db6f37a9f421ed778",
            "580efc6b874f4837ad9e16d5838ee674",
            "2fbdac598dbf421f9b61db747d875d38",
            "2788adcc497d43c3930581603e7f2bde",
            "50222576594a441b9e0c80a83a532208",
            "f3d71bf03f3b4fc9a3077a5eaa161c2a",
            "9179cb7380364500a6e5fe365603a13c",
            "be5eef5b6b7f4887b5786b73a470ce56",
            "2e336356d3374ecb8a0b234224621527",
            "bdba6a27a36c4f10a381a4b16ca47cb0",
            "a9bfeb59b992471c8e988ff4c637a883",
            "8db0e2b4c5674590b043e09466c70243",
            "f3e50e7b0d054e60947df5f6997baeaf",
            "071316cd47894835b403905a21478001",
            "f64bd38d9a8244528961bb20f7b57217",
            "b543bf90b81f4a94af27bcbb89199a9a",
            "b7ff2ba2441342eea769cf36dd8f1b45",
            "9a41522255064abaa8b785f57285c7e0",
            "e3f2ede453644debbf557ec4fe4de85a",
            "07e1d92578d748049277b9750e89068c",
            "3ab7d05741b24625992e0a624887d589",
            "386ed87d00fb4827b608046eb2e8a780",
            "4ae67be23eb84847851b88ebf0923406",
            "5122533cb2574df7b9d371c63eb6b373",
            "b4ce1c1127054bf193daac1176ba1b75"
          ]
        },
        "id": "yLhyojxAEZEW",
        "outputId": "f3137da3-b5bf-430d-ae80-601069347fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting prompt in train dataset (num_proc=12):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e725475f7fc4d38b89c5319d1b6d976"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to train dataset (num_proc=12):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2788adcc497d43c3930581603e7f2bde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset (num_proc=12):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f64bd38d9a8244528961bb20f7b57217"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 125\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 4,884,480 of 139,399,488 (3.50% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 06:03, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.690100</td>\n",
              "      <td>0.016908</td>\n",
              "      <td>0.010589</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.006318</td>\n",
              "      <td>-826.068970</td>\n",
              "      <td>-628.409546</td>\n",
              "      <td>9.897448</td>\n",
              "      <td>9.846713</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.665100</td>\n",
              "      <td>0.105974</td>\n",
              "      <td>0.046287</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.059687</td>\n",
              "      <td>-797.069336</td>\n",
              "      <td>-490.965271</td>\n",
              "      <td>9.850774</td>\n",
              "      <td>9.826957</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.672600</td>\n",
              "      <td>0.232395</td>\n",
              "      <td>0.172771</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.059624</td>\n",
              "      <td>-810.577515</td>\n",
              "      <td>-711.990051</td>\n",
              "      <td>9.739737</td>\n",
              "      <td>9.483553</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.642100</td>\n",
              "      <td>0.315385</td>\n",
              "      <td>0.182240</td>\n",
              "      <td>0.662500</td>\n",
              "      <td>0.133145</td>\n",
              "      <td>-878.228149</td>\n",
              "      <td>-626.377991</td>\n",
              "      <td>9.572217</td>\n",
              "      <td>9.821025</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.620400</td>\n",
              "      <td>0.375560</td>\n",
              "      <td>0.193200</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.182361</td>\n",
              "      <td>-815.394897</td>\n",
              "      <td>-654.939941</td>\n",
              "      <td>9.582856</td>\n",
              "      <td>9.491736</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.630200</td>\n",
              "      <td>0.394063</td>\n",
              "      <td>0.220260</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.173803</td>\n",
              "      <td>-779.254761</td>\n",
              "      <td>-610.480835</td>\n",
              "      <td>9.518569</td>\n",
              "      <td>9.376617</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.613200</td>\n",
              "      <td>0.537292</td>\n",
              "      <td>0.297393</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.239899</td>\n",
              "      <td>-875.328430</td>\n",
              "      <td>-700.653259</td>\n",
              "      <td>9.604898</td>\n",
              "      <td>9.532171</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.611300</td>\n",
              "      <td>0.494367</td>\n",
              "      <td>0.249861</td>\n",
              "      <td>0.612500</td>\n",
              "      <td>0.244505</td>\n",
              "      <td>-814.120239</td>\n",
              "      <td>-603.798401</td>\n",
              "      <td>9.527124</td>\n",
              "      <td>9.289549</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.608700</td>\n",
              "      <td>0.443536</td>\n",
              "      <td>0.211955</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.231581</td>\n",
              "      <td>-709.067810</td>\n",
              "      <td>-586.446167</td>\n",
              "      <td>9.566622</td>\n",
              "      <td>9.286982</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.626600</td>\n",
              "      <td>0.507469</td>\n",
              "      <td>0.283758</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.223711</td>\n",
              "      <td>-765.637878</td>\n",
              "      <td>-636.685425</td>\n",
              "      <td>9.450400</td>\n",
              "      <td>9.354359</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.577600</td>\n",
              "      <td>0.546687</td>\n",
              "      <td>0.218207</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.328480</td>\n",
              "      <td>-892.647461</td>\n",
              "      <td>-589.758911</td>\n",
              "      <td>9.459620</td>\n",
              "      <td>9.426493</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.591700</td>\n",
              "      <td>0.542867</td>\n",
              "      <td>0.251764</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.291103</td>\n",
              "      <td>-830.408325</td>\n",
              "      <td>-583.346680</td>\n",
              "      <td>9.599452</td>\n",
              "      <td>9.522871</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n",
            "TrainOutput(global_step=125, training_loss=0.6272734756469727, metrics={'train_runtime': 372.4231, 'train_samples_per_second': 2.685, 'train_steps_per_second': 0.336, 'total_flos': 0.0, 'train_loss': 0.6272734756469727, 'epoch': 1.0})\n",
            "Saved DPO LoRA adapter -> smollm2_dpo_adapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean reload for inference: base (4-bit) + DPO LoRA adapter, then sample from dataset\n",
        "import os, random, torch\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "\n",
        "base_id     = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "adapter_dir = \"smollm2_dpo_adapter\"\n",
        "assert os.path.isdir(adapter_dir), \"Adapter folder not found. Run the DPO training cell first.\"\n",
        "\n",
        "# Recreate tokenizer + base in 4-bit\n",
        "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "policy_base, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = base_id,\n",
        "    max_seq_length = 2048,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = True,\n",
        ")\n",
        "\n",
        "# Attach LoRA adapter (policy)\n",
        "model = PeftModel.from_pretrained(policy_base, adapter_dir)\n",
        "model.eval()\n",
        "\n",
        "# Tokenizer safety defaults\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "def generate(prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# Pick a random prompt from the DPO subset if available; otherwise use a fallback\n",
        "if \"small_ds\" in globals() and len(small_ds) > 0:\n",
        "    i = random.randint(0, len(small_ds) - 1)\n",
        "    demo = small_ds[i]\n",
        "    prompt = demo[\"prompt\"]\n",
        "    chosen = demo[\"chosen\"][:300]\n",
        "    rejected = demo[\"rejected\"][:300]\n",
        "else:\n",
        "    prompt = \"Question:\\nWhat are the benefits of unit testing in software development?\\n\\nAnswer:\"\n",
        "    chosen = rejected = \"(no ground truth available in this quick test)\"\n",
        "\n",
        "print(\"=== PROMPT ===\\n\", prompt[:600], \"...\\n\")\n",
        "print(\"=== (dataset) CHOSEN (truncated) ===\\n\", chosen, \"\\n\")\n",
        "print(\"=== (dataset) REJECTED (truncated) ===\\n\", rejected, \"\\n\")\n",
        "\n",
        "gen = generate(prompt, max_new_tokens=220)\n",
        "print(\"=== POLICY OUTPUT ===\\n\", gen, \"\\n\")\n",
        "\n",
        "# Convenience: show only the continuation after 'Answer:' if present\n",
        "tag = \"Answer:\"\n",
        "if tag in gen:\n",
        "    print(\"=== POLICY CONTINUATION ===\\n\", gen.split(tag, 1)[-1].strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbRDf3BjGTiW",
        "outputId": "7f48d826-5276-4c3a-ae51-b8bedf284bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n",
            "=== PROMPT ===\n",
            " Question:\n",
            "<p>I don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "Answer: ...\n",
            "\n",
            "=== (dataset) CHOSEN (truncated) ===\n",
            " <p>I stumbled across this forum/group, <a href=\"https://forum.prusaprinters.org/forum/english-forum-original-prusa-i3-mmu2s-mmu2/\" rel=\"nofollow noreferrer\">Original Prusa i3 MMU2S &amp; MMU2</a>, amongst all of the other <a href=\"https://forum.prusaprinters.org\" rel=\"nofollow noreferrer\">Prusa prin \n",
            "\n",
            "=== (dataset) REJECTED (truncated) ===\n",
            " <p>There is a lot of activity on Reddit related to 3D printing and the Prusa printers.</p> \n",
            "\n",
            "=== POLICY OUTPUT ===\n",
            " Question:\n",
            "<p>I don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "Answer:\n",
            "\n",
            "<p>You don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "<p>The only question that I have is this:</p>\n",
            "\n",
            "<p>What is the best way to use a keyboard and/or mouse?</p>\n",
            "\n",
            "<p>It's not a problem, but I don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "<p>No, I don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "<p>I'm not sure. I have a couple of ideas.\n",
            "\n",
            "<p>1. Don't use a keyboard. (I think this is a problem). I think a mouse is better. I \n",
            "\n",
            "=== POLICY CONTINUATION ===\n",
            " <p>You don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "<p>The only question that I have is this:</p>\n",
            "\n",
            "<p>What is the best way to use a keyboard and/or mouse?</p>\n",
            "\n",
            "<p>It's not a problem, but I don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "<p>No, I don't want to ask off-topic and opinion questions here, but I would like to find a cadre of others dialing in their devices. Any ideas?</p>\n",
            "\n",
            "<p>I'm not sure. I have a couple of ideas.\n",
            "\n",
            "<p>1. Don't use a keyboard. (I think this is a problem). I think a mouse is better. I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Merge DPO LoRA weights into a full model and export to GGUF for Ollama\n",
        "import os, glob, torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "base_id      = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "adapter_dir  = \"smollm2_dpo_adapter\"\n",
        "merged_dir   = \"smollm2_dpo_merged_16bit\"\n",
        "gguf_dir     = \"gguf_export_dpo_q8\"\n",
        "\n",
        "assert os.path.isdir(adapter_dir), \"Adapter folder not found. Run the DPO training cell first.\"\n",
        "\n",
        "# If 'model' and 'tokenizer' aren't in RAM (fresh runtime), reconstruct them:\n",
        "if \"model\" not in globals() or \"tokenizer\" not in globals():\n",
        "    policy_base, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name     = base_id,\n",
        "        max_seq_length = 2048,\n",
        "        dtype          = torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        load_in_4bit   = True,\n",
        "    )\n",
        "    from peft import PeftModel\n",
        "    model = PeftModel.from_pretrained(policy_base, adapter_dir)\n",
        "\n",
        "# 1) Try Unsloth-native merge (preferred)\n",
        "merged_ok = False\n",
        "if hasattr(model, \"save_pretrained_merged\"):\n",
        "    try:\n",
        "        model.save_pretrained_merged(merged_dir, tokenizer, save_method=\"merged_16bit\")\n",
        "        print(\"Merged LoRA ‚Üí\", merged_dir, \"(Unsloth merged_16bit).\")\n",
        "        merged_ok = True\n",
        "    except Exception as e:\n",
        "        print(\"Unsloth merged_16bit failed, will try PEFT fallback:\", e)\n",
        "\n",
        "# 2) Fallback: PEFT merge_and_unload\n",
        "if not merged_ok:\n",
        "    try:\n",
        "        from peft import PeftModel\n",
        "        merged = model.merge_and_unload()\n",
        "        os.makedirs(merged_dir, exist_ok=True)\n",
        "        merged.save_pretrained(merged_dir)\n",
        "        tokenizer.save_pretrained(merged_dir)\n",
        "        print(\"Merged LoRA ‚Üí\", merged_dir, \"(PEFT fallback).\")\n",
        "        merged_ok = True\n",
        "    except Exception as e:\n",
        "        print(\"PEFT merge fallback failed:\", e)\n",
        "\n",
        "# 3) Export to GGUF for Ollama (Q8_0)\n",
        "if merged_ok:\n",
        "    os.makedirs(gguf_dir, exist_ok=True)\n",
        "    try:\n",
        "        merged_model, merged_tok = FastLanguageModel.from_pretrained(\n",
        "            model_name      = merged_dir,\n",
        "            max_seq_length  = 2048,\n",
        "            dtype           = torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            load_in_4bit    = False,\n",
        "            full_finetuning = False,\n",
        "        )\n",
        "\n",
        "        merged_model.save_pretrained_gguf(\n",
        "            gguf_dir,\n",
        "            merged_tok,\n",
        "            quantization_method=\"q8_0\",\n",
        "        )\n",
        "        print(\"Saved GGUF to:\", gguf_dir)\n",
        "\n",
        "        # Write a simple Ollama Modelfile\n",
        "        ggufs = glob.glob(os.path.join(gguf_dir, \"*.gguf\"))\n",
        "        gguf_name = os.path.basename(ggufs[0]) if ggufs else \"model-Q8_0.gguf\"\n",
        "        with open(os.path.join(gguf_dir, \"Modelfile\"), \"w\") as f:\n",
        "            f.write(f\"FROM ./{gguf_name}\\n\")\n",
        "            f.write(\"PARAMETER temperature 0.7\\n\")\n",
        "            f.write(\"PARAMETER top_p 0.9\\n\")\n",
        "            f.write(\"TEMPLATE \\\"{{ .System }}\\\\n\\\\n{{ .Prompt }}\\\"\\n\")\n",
        "        print(\"Wrote Modelfile ‚Üí\", os.path.join(gguf_dir, \"Modelfile\"))\n",
        "\n",
        "        print(\"\\nNext steps (locally):\")\n",
        "        print(f\"cd {gguf_dir}\")\n",
        "        print(\"ollama create smollm2-dpo -f Modelfile\")\n",
        "        print(\"ollama run smollm2-dpo\")\n",
        "    except Exception as e:\n",
        "        print(\"GGUF export skipped or failed:\", e)\n",
        "else:\n",
        "    print(\"Merge did not succeed; skipping GGUF export.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daYIdfNpIGW1",
        "outputId": "919cafcf-0662-4610-d922-4247c97a90fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/unsloth_zoo/saving_utils.py:969: UserWarning: Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\n",
            "  warnings.warn(\"Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged LoRA ‚Üí smollm2_dpo_merged_16bit (Unsloth merged_16bit).\n",
            "GGUF export skipped or failed: Unsloth: No config file found - are you sure the `model_name` is correct?\n",
            "If you're using a model on your local device, confirm if the folder location exists.\n",
            "If you're using a HuggingFace online model, check if it exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agEBR3KsIbSr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}