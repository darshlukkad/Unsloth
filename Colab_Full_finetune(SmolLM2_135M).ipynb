{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPq7KGgMmHNH6KlQGambVu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshlukkad/Unsloth/blob/main/Colab_Full_finetune(SmolLM2_135M).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fPgJgmSuBJX",
        "outputId": "b12b1a0b-1c20-4c18-8a6d-67a03030816b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU name: Tesla T4\n",
            "Sun Nov  9 22:28:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0             29W /   70W |     142MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# --- Install core deps for Unsloth finetuning (Colab-friendly) ---\n",
        "%pip -q install -U unsloth transformers trl datasets accelerate peft bitsandbytes einops evaluate sentencepiece\n",
        "\n",
        "import torch, os, sys, platform, subprocess\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "# Show GPU if available\n",
        "try:\n",
        "    import torch\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print(\"GPU check error:\", e)\n",
        "\n",
        "# Optional: view CUDA details (will print an error on CPU-only runtimes)\n",
        "!nvidia-smi || echo \"No NVIDIA GPU detected\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment & core imports for full finetuning\n",
        "import torch, os\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"dtype:\", dtype, \"| device:\", device)\n",
        "MAX_SEQ_LEN = 2048\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5ARUl3NvUIV",
        "outputId": "2c017356-749e-421a-ac3d-cfaee5b09db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype: torch.float16 | device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Repair path for NameError: DEVICE_TYPE_TORCH ---\n",
        "# 1) Set Unsloth flags BEFORE importing it\n",
        "import os, sys, importlib, shutil\n",
        "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"          # disable compiled kernels\n",
        "os.environ[\"UNSLOTH_STABLE_DOWNLOADS\"] = \"1\"         # stabilize hf downloads\n",
        "print(\"UNSLOTH_COMPILE_DISABLE =\", os.environ[\"UNSLOTH_COMPILE_DISABLE\"])\n",
        "\n",
        "# 2) Force-reinstall Unsloth + Zoo (common fix from docs)\n",
        "#    (Also keeps your existing torch/transformers, but refreshes Unsloth bits.)\n",
        "%pip -q install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
        "\n",
        "# 3) Purge previously-imported modules so Unsloth can patch cleanly\n",
        "to_purge = tuple([\"unsloth\", \"transformers\", \"trl\", \"peft\"])\n",
        "purged = [m for m in list(sys.modules) if m.startswith(to_purge)]\n",
        "for m in purged:\n",
        "    del sys.modules[m]\n",
        "print(\"Purged modules:\", len(purged))\n",
        "\n",
        "# 4) Import Unsloth FIRST, then the rest (required for patching)\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "# 5) Recreate env vars & settings\n",
        "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_SEQ_LEN = 2048\n",
        "print(\"dtype:\", dtype, \"| device:\", device)\n",
        "\n",
        "# 6) Load model for FULL finetuning (no LoRA)\n",
        "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name      = model_id,\n",
        "    max_seq_length  = MAX_SEQ_LEN,\n",
        "    dtype           = dtype,\n",
        "    load_in_4bit    = False,       # full precision for tiny model\n",
        "    full_finetuning = True,        # full-parameter finetune\n",
        ")\n",
        "\n",
        "# padding defaults\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"Loaded model:\", model_id)\n",
        "print(\"Device:\", model.device)\n",
        "!nvidia-smi || echo \"No NVIDIA GPU detected\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851,
          "referenced_widgets": [
            "b1875e431300429fa1e0d68263375e77",
            "c7ab43b702c646539e6ffc2fead6d437",
            "44fbbf28dd8e40aa8eb685d2ce317ef9",
            "06cf5289422c4221ba29ed197244085e",
            "d2db45aefa9e4da1818eb68d53b99e74",
            "53489e6d1dda4db7874942467155a856",
            "596646adce5a4bcba63d39e8819024f0",
            "a3c5b90cecc3478195291b74c64aaa32",
            "f3607a95f701407492e19c56069886b0",
            "5bbc721b439740f18b7be14ec34591d9",
            "07165710eba84e90a4cc7e54fe724de6",
            "70ceed46876248ae867dc1771d67bfd2",
            "2a4891ee626440339c3f251ff9e20ca1",
            "3d12bb64fdcd4da5998ea051beab99fb",
            "51cbbe45ee3f43b2a14b7500c648e194",
            "7246dc3758694fad8e8b4c407805615b",
            "c1a54f46c17f47f1b78b431840aa1dd1",
            "349f1648a9e04445863e923c7a4ce23c",
            "ff4932f182bd4029b3078bc003af3d2d",
            "c7d5d3f3b9374a53b70db7f427c53005",
            "4b7652d31bb24ce9874da174437a33c1",
            "55d7c25dcee24fcb966d75d7bcf052d8",
            "6e9d741a927145c4a945398d5c9ed009",
            "edbdf73cd152414287b1fc170d7b7d00",
            "fffcbc40a6eb4f89950dd781d907af00",
            "70b92322cf5149f2ad7659e034f91da2",
            "99a04d7b1126435f90ad048823aeb17b",
            "46013f5b17e444d4ae69798a9bc341b0",
            "7a1263e6c1184fc097f48ec105fe5891",
            "61f2627aa8884f6793352eef345199db",
            "03d9bbe48ff246dbaeea565a9e63c740",
            "57bfe483f8054bbebd41a213411d082f",
            "8c4d92db435446c4b17206ef66027e27",
            "cad6eceec5c242ef8244f35a186dda90",
            "156158c117ee43048a2ed4b5e028abd3",
            "bfe39f7a89a94196aa7595a6399c85cf",
            "f8a8e7d7a9914d9d911dfb63dafd2b4d",
            "4c4040f9cec74a89be66094e894f9e60",
            "99ebc5f63d5e4f5a82cc0431bdcdcec6",
            "e49e87d8cf2c4c2095674413071ce13a",
            "ba87d3d58b1f45e4aedca877c4f2a957",
            "9f0c232005c945b3ae1eb852aea10fd0",
            "8cd53067cdd043aba6e2f8fe29c9f173",
            "7bb87b3581e74a6b8f4bcae3b6603b51",
            "2fe50ec3e3864a9d8e9ab63ee962bd4a",
            "293333ac0f754e1cb2c1bc35740e7ff7",
            "af7acb8fa26b4fe0beb7c48621a40603",
            "f487569e82e740e789a9776ee6e7b444",
            "dfa69507c9124f22bc3684289b02d8e0",
            "d55ba5965378448da39ca8f7fbd4cd40",
            "0ca4ccdf5935444da40c5dd42be7e791",
            "0b18109f80e24324b1273adc977603d5",
            "ea29f491e2794765b70e281eba776d6a",
            "b309e833e9fb44e9a2eea0634b2c6173",
            "5f8fc1521d3a49589ddb48850beccf06",
            "0aae5edb601c4527bf302c6a38841774",
            "0a6fe5ea9d564a1888985fbb3cffc961",
            "fe14895b779c42b2a38dc438d6612af6",
            "0be8d0fe65e74d7898e4704c080f639a",
            "34c5bb9b6669469aa78bfe7b35d65ed5",
            "abbfc8985c5847669c8243ec55dd87ea",
            "d91a1d8b02064da89ea1478630b72ff6",
            "185799d573ba4aac82af148b46691e5e",
            "69bc91abe25b45d5924b9439f120ea97",
            "5b85bd143008435aa76920c735445a9d",
            "d743ff1a8d604d08ae31465c07b83831"
          ]
        },
        "id": "h8qZg67EvUyq",
        "outputId": "3a832ec1-a50d-4eb2-8e5d-2bd69bfcb2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNSLOTH_COMPILE_DISABLE = 1\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m354.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m374.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPurged modules: 501\n",
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "dtype: torch.float16 | device: cuda\n",
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1875e431300429fa1e0d68263375e77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70ceed46876248ae867dc1771d67bfd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e9d741a927145c4a945398d5c9ed009"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cad6eceec5c242ef8244f35a186dda90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fe50ec3e3864a9d8e9ab63ee962bd4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0aae5edb601c4527bf302c6a38841774"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n",
            "Loaded model: HuggingFaceTB/SmolLM2-135M\n",
            "Device: cuda:0\n",
            "Sun Nov  9 22:36:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0             31W /   70W |     706MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load instruction-style dataset with preformatted `text` field\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
        "print(ds)\n",
        "print(\"Sample row:\\n\", ds[0][\"text\"][:600], \"...\\n\")\n",
        "\n",
        "# Use a tiny subset for a quick smoke test; increase later for your full run\n",
        "train_ds = ds.select(range(min(200, len(ds))))\n",
        "len(train_ds), train_ds[0][\"text\"][:200]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "e8449335625b4576a4900f545554860c",
            "7521e22334be46d8a5fa1f17f572ccae",
            "87cb7788149741cf928185754ef1f453",
            "d7f2f71ab99b4205ae7e5ad45a16480f",
            "f491f573d4564157963d40f996d8107c",
            "8859500f4b374cdb8c5055f4e39e27cf",
            "f0f3b64a13fe47ddb05da97f4b11d73c",
            "65c2ac261b2046f49a3d0b84572d0d0a",
            "c09777ba69104f94a12d9f25813d8cbe",
            "546606f300cd4b87b2d0634993393910",
            "1cdbe13df1324c1d9d675e6b297f51b9",
            "34e1d647effa4128b6ff9593297550f2",
            "5f7ce3976dd94de28350bf3d405490fc",
            "d680ba78b3cc44bd9aa3c18a2f27c524",
            "1b11653bc4e648ab8918cb26c07b1c44",
            "04dc5a3971a54ec3b1a2f7b027c7d325",
            "19a7febaba9e40ec9d385edba86b6c2e",
            "6679fada6ffc4a008775381989c412de",
            "ff7ab9da709c45d080bcc24151ca162c",
            "f436ed4d928d4dde92433a79070ce52f",
            "a5ce92fe1101421ba65148317f54bb4a",
            "8c43da22fc2c4c09bcc72eedece98b17",
            "db3f350e46354b02b9f11883446fa49a",
            "3025c228f026489398c7220efea136fd",
            "14ca5e99563842d0923a64b60809e08f",
            "f65b34f2f5654799a797a1c2560122dc",
            "8621c850616b4ab1b50909aea824dfbd",
            "dd150c6da15d465dbd331d40c3ef90ff",
            "0bdfc1825926433a8bbb75e18e1575f7",
            "e6292bc9d7ea4b79ac05ffb7c4d88133",
            "9b3022380e2340f6a31eba820461b827",
            "ebb34fa711f94ca4bff96df48794ad9c",
            "b6226d2fac824fabba64f3a74c19431b"
          ]
        },
        "id": "f1HqYYC8ve_k",
        "outputId": "7feacbfb-ec2c-4e90-f439-b820d7406a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8449335625b4576a4900f545554860c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001-9ad84bb9cf65a4(‚Ä¶):   0%|          | 0.00/967k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34e1d647effa4128b6ff9593297550f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db3f350e46354b02b9f11883446fa49a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Sample row:\n",
            " <s>[INST] Me gradu√© hace poco de la carrera de medicina ¬øMe podr√≠as aconsejar para conseguir r√°pidamente un puesto de trabajo? [/INST] Esto vale tanto para m√©dicos como para cualquier otra profesi√≥n tras finalizar los estudios aniversarios y mi consejo ser√≠a preguntar a cu√°ntas personas haya conocido mejor. En este caso, mi primera opci√≥n ser√≠a hablar con otros profesionales m√©dicos, echar curr√≠culos en hospitales y cualquier centro de salud. En paralelo, trabajar√≠a por mejorar mi marca personal como m√©dico mediante un blog o formas digitales de comunicaci√≥n como los v√≠deos. Y, para mejorar la ...\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200,\n",
              " '<s>[INST] Me gradu√© hace poco de la carrera de medicina ¬øMe podr√≠as aconsejar para conseguir r√°pidamente un puesto de trabajo? [/INST] Esto vale tanto para m√©dicos como para cualquier otra profesi√≥n t')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full-parameter finetuning with plain HF Trainer (no LoRA)\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# Tokenize the dataset (simple truncation to a fixed context length)\n",
        "MAX_LENGTH = 512\n",
        "def tok_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=False,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "\n",
        "tokenized_train = train_ds.map(tok_fn, batched=True, remove_columns=train_ds.column_names)\n",
        "print(tokenized_train[0].keys())\n",
        "\n",
        "# For causal LM (next-token prediction), MLM must be False\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Trainer args: memory-friendly 8-bit AdamW (bitsandbytes) on Colab T4\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"outputs_fullft_smollm2_hf\",\n",
        "    per_device_train_batch_size=8,     # if OOM: try 4 or 2\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,                 # increase for better results\n",
        "    learning_rate=5e-4,                 # tiny models can tolerate higher LR\n",
        "    fp16=True,                          # T4 prefers fp16\n",
        "    bf16=False,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_bnb_8bit\",             # 8-bit optimizer (bitsandbytes)\n",
        ")\n",
        "\n",
        "# Some models prefer disabling cache during training\n",
        "model.config.use_cache = False\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "train_out = trainer.train()\n",
        "print(train_out)\n",
        "\n",
        "# Save final model + tokenizer\n",
        "trainer.save_model(\"smollm2_fullft_model\")\n",
        "tokenizer.save_pretrained(\"smollm2_fullft_model\")\n",
        "print(\"Saved full-FT model to: smollm2_fullft_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "51760a2fae3f4412a515ba3330b9dfde",
            "72a11a6cada54d908b1a874b8bc59e17",
            "888a2b0593de46ae8b295087900f54e7",
            "d10bf16fed384546a2f485415af4032a",
            "7dec01d7085c4c839e50e97c67d7a172",
            "d67aa5713f544fe29e8e6a583dfa6d38",
            "375895a721a147878605bfa34cb03b75",
            "b9897671cc484835a2b167da06ac830e",
            "51a1901727d645bc99a6d3559f64e43c",
            "cdf2e4b9f74e4d5bb217c41b97e16751",
            "08daa049f13348a4a08d150bf0a25b24"
          ]
        },
        "id": "7ME6FFmRxTWf",
        "outputId": "4d742792-5945-498d-8e7e-69db90fdca98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51760a2fae3f4412a515ba3330b9dfde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 200 | Num Epochs = 1 | Total steps = 25\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 134,515,008 of 134,515,008 (100.00% trained)\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 200 | Num Epochs = 1 | Total steps = 25\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 134,515,008 of 134,515,008 (100.00% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:17, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.189200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.210100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "TrainOutput(global_step=25, training_loss=2.2229506301879884, metrics={'train_runtime': 21.3231, 'train_samples_per_second': 9.379, 'train_steps_per_second': 1.172, 'total_flos': 65215719005184.0, 'train_loss': 2.2229506301879884, 'epoch': 1.0})\n",
            "Saved full-FT model to: smollm2_fullft_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a quick response from the fine-tuned model\n",
        "import os, torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# If we saved a model dir (from the previous cell), load from disk for a clean test.\n",
        "if os.path.isdir(\"smollm2_fullft_model\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"smollm2_fullft_model\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"smollm2_fullft_model\",\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=None,\n",
        "    ).to(device)\n",
        "    print(\"Reloaded model from: smollm2_fullft_model\")\n",
        "\n",
        "# Speed up generation\n",
        "if hasattr(model.config, \"use_cache\"):\n",
        "    model.config.use_cache = True\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Guanaco uses an instruction style like below\n",
        "prompt = (\n",
        "    \"### Instruction:\\n\"\n",
        "    \"Write a short, friendly greeting in 1‚Äì2 sentences.\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    gen_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Show full text and the model-only continuation (after '### Response:')\n",
        "print(\"==== Full output ====\\n\", text, \"\\n\")\n",
        "resp_start = text.find(\"### Response:\")\n",
        "print(\"==== Model continuation ====\\n\", text[resp_start + len(\"### Response:\"):].strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBrFMNKaxw9Z",
        "outputId": "ff83b94f-f28c-4f7e-cce6-aacd6dc1cd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloaded model from: smollm2_fullft_model\n",
            "==== Full output ====\n",
            " ### Instruction:\n",
            "Write a short, friendly greeting in 1‚Äì2 sentences.\n",
            "\n",
            "### Response:\n",
            "\n",
            "Hello! I'm glad to hear from you! This is a great opportunity to learn more about the world of health care and make a positive impact on the lives of people who depend on it.\n",
            "\n",
            "I am happy to answer any questions you may have and to provide you with a list of resources and online tools that can help you learn more about the field of health care.\n",
            "\n",
            "Here are some resources and tools that might be helpful:\n",
            "\n",
            "* Health Information Exchange: A digital platform that allows users to exchange information on health data and services.\n",
            "* Patient Centered Care: A model of health care delivery that emphasizes the \n",
            "\n",
            "==== Model continuation ====\n",
            " Hello! I'm glad to hear from you! This is a great opportunity to learn more about the world of health care and make a positive impact on the lives of people who depend on it.\n",
            "\n",
            "I am happy to answer any questions you may have and to provide you with a list of resources and online tools that can help you learn more about the field of health care.\n",
            "\n",
            "Here are some resources and tools that might be helpful:\n",
            "\n",
            "* Health Information Exchange: A digital platform that allows users to exchange information on health data and services.\n",
            "* Patient Centered Care: A model of health care delivery that emphasizes the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate Unsloth chat templates and generate with the fine-tuned model\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "\n",
        "# If you restarted, reload from the saved directory\n",
        "import os\n",
        "if not (\"model\" in globals() and \"tokenizer\" in globals()):\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"smollm2_fullft_model\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"smollm2_fullft_model\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Get two popular templates\n",
        "tok_llama  = get_chat_template(tokenizer, chat_template=\"llama\")\n",
        "tok_alpaca = get_chat_template(tokenizer, chat_template=\"alpaca\")\n",
        "\n",
        "# A tiny conversation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, friendly assistant.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"Write a cheerful greeting in one or two sentences.\"},\n",
        "]\n",
        "\n",
        "# Apply templates (string prompts)\n",
        "prompt_llama  = tok_llama.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "prompt_alpaca = tok_alpaca.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(\"=== LLaMA-style prompt ===\\n\", prompt_llama[:400], \"...\\n\")\n",
        "print(\"=== Alpaca-style prompt ===\\n\", prompt_alpaca[:400], \"...\\n\")\n",
        "\n",
        "# Generate with both to hear formatting differences\n",
        "def gen(prompt, max_new_tokens=120):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n=== Output (LLaMA template) ===\\n\", gen(prompt_llama))\n",
        "print(\"\\n=== Output (Alpaca template) ===\\n\", gen(prompt_alpaca))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33veJ6HGyDvX",
        "outputId": "098a1cae-d0dc-4c35-cf62-8d48737f9a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model does not have a padding token! Will use pad_token = <|endoftext|>.\n",
            "Model does not have a padding token! Will use pad_token = <|endoftext|>.\n",
            "=== LLaMA-style prompt ===\n",
            " <|endoftext|>You are a concise, friendly assistant.\n",
            "\n",
            "### Instruction:\n",
            "Write a cheerful greeting in one or two sentences.\n",
            "\n",
            "### Response:\n",
            " ...\n",
            "\n",
            "=== Alpaca-style prompt ===\n",
            " <|endoftext|>You are a concise, friendly assistant.\n",
            "\n",
            "### Instruction:\n",
            "Write a cheerful greeting in one or two sentences.\n",
            "\n",
            "### Response:\n",
            " ...\n",
            "\n",
            "\n",
            "=== Output (LLaMA template) ===\n",
            " You are a concise, friendly assistant.\n",
            "\n",
            "### Instruction:\n",
            "Write a cheerful greeting in one or two sentences.\n",
            "\n",
            "### Response:\n",
            "Dear [Name],\n",
            "\n",
            "I am happy to hear from you and I hope you enjoy our chat.\n",
            "\n",
            "Your name is [Your Name], and I am happy to meet you. I am looking forward to learning more about you and the world around you.\n",
            "\n",
            "I hope you have a wonderful day!\n",
            "\n",
            "Thank you!\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "\n",
            "Thank you!\n",
            "\n",
            "I hope you have a great day!\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "\n",
            "Thank you!\n",
            "\n",
            "I am looking forward to meeting you and the world\n",
            "\n",
            "=== Output (Alpaca template) ===\n",
            " You are a concise, friendly assistant.\n",
            "\n",
            "### Instruction:\n",
            "Write a cheerful greeting in one or two sentences.\n",
            "\n",
            "### Response:\n",
            "Here are some suggestions:\n",
            "\n",
            "Hello!\n",
            "I hope you're having a great day!\n",
            "\n",
            "Thank you!\n",
            "I appreciate your support!\n",
            "I hope you enjoy your stay here.\n",
            "I hope you have a wonderful time in the city.\n",
            "I hope you have fun exploring the city!\n",
            "\n",
            "Cheers!\n",
            "(Your laughter and enthusiasm are contagious!)\n",
            "I hope you enjoy your stay here.\n",
            "I hope you have a wonderful time in the city.\n",
            "I hope you have fun exploring the city!\n",
            "\n",
            "Thank you!\n",
            "I appreciate your support!\n",
            "I hope\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MXqP6EFy3cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}